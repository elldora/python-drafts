{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to work with Parsivar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import pyarrow\n",
    "\n",
    "path = 'e:/data/advertor_phoneless_imbalance_r60_p40_1400/'\n",
    "\n",
    "file_path_dict = {\n",
    "    'advertor': path+'Advertor_Advers_Imbalance_R60_P40_1400_1.parquet'}\n",
    "\n",
    "df_list = []\n",
    "for source, filepth in file_path_dict.items():\n",
    "    df = pd.read_parquet(filepth,  engine='pyarrow')\n",
    "    df_list.append(df[['AdverText', 'City_Id', 'IsRealtor_New']])\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "print(df.iloc[0])\n",
    "df.head()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from parsivar import Normalizer, Tokenizer, FindStems, POSTagger\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    normalizer = Normalizer()\n",
    "    tokenizer = Tokenizer()\n",
    "    sentence = normalizer.normalize(sentence)\n",
    "    # sentence = tokenizer.tokenize_words(sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def arToPersianNumb(number):\n",
    "    dic = {\n",
    "        '١': '۱',\n",
    "        '٢': '۲',\n",
    "        '٣': '۳',\n",
    "        '٤': '۴',\n",
    "        '٥': '۵',\n",
    "        '٦': '۶',\n",
    "        '٧': '۷',\n",
    "        '٨': '۸',\n",
    "        '٩': '۹',\n",
    "        '٠': '۰',\n",
    "    }\n",
    "    return multiple_replace(dic, number)\n",
    "\n",
    "\n",
    "def arToPersianChar(userInput):\n",
    "    dic = {\n",
    "        'ك': 'ک',\n",
    "        'دِ': 'د',\n",
    "        'بِ': 'ب',\n",
    "        'زِ': 'ز',\n",
    "        'ذِ': 'ذ',\n",
    "        'شِ': 'ش',\n",
    "        'سِ': 'س',\n",
    "        'ى': 'ی',\n",
    "        'ي': 'ی'\n",
    "    }\n",
    "    return multiple_replace(dic, userInput)\n",
    "\n",
    "\n",
    "def multiple_replace(dic, text):\n",
    "    pattern = \"|\".join(map(re.escape, dic.keys()))\n",
    "    return re.sub(pattern, lambda m: dic[m.group()], str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def clean_all(document):\n",
    "    clean = []\n",
    "    for sentence in document:\n",
    "        sentence = clean_sentence(sentence)\n",
    "        print(sentence)\n",
    "        clean.append(sentence)\n",
    "    return (clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df[:100]['AdverText']\n",
    "print(doc.tolist)\n",
    "prep_senctences = clean_all(doc)\n",
    "print(type(prep_senctences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer.normalize(doc[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(prep_senctences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prep_senctences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsivar import Normalizer, Tokenizer, FindStems, POSTagger\n",
    "\n",
    "\n",
    "sentences = df[:20]['AdverText']\n",
    "print(type(sentences))\n",
    "\n",
    "normalizer = Normalizer(half_space_char=\"\\u200c\", statistical_space_correction=True)\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "normalized_sens = []\n",
    "for sen in sentences:\n",
    "    normalized_sens.append(normalizer.normalize(sen))\n",
    "\n",
    "print(type(normalized_sens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer()\n",
    "# stemmer = FindStems()\n",
    "# tagger = POSTagger()\n",
    "# tokenized_sen = tokenizer.tokenize_sentences(normalized_sen)\n",
    "# words = tokenizer.tokenize_words(normalized_sen)\n",
    "# stemmized_sen = stemmer.convert_to_stem('میفروشیم') \n",
    "# tagged_sen = POSTagger(tagging_model=\"wapiti\")\n",
    "\n",
    "# print(sentences)\n",
    "# print(normalized_sen)\n",
    "# print(tokenized_sen)\n",
    "# print(words)\n",
    "# print(stemmized_sen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tensorflow-keras')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6792be08d248a6a913297fb0a515ddc17689372100569e91a189e4d8039a3a50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
